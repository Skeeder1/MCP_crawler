# üìò Guide Technique - MCP Hub Crawler

> Documentation technique compl√®te de l'architecture, du fonctionnement du code et des interactions entre composants.

**Date**: 21 Novembre 2025
**Version**: 2.0 (apr√®s r√©organisation)

---

## Table des Mati√®res

1. [Vue d'Ensemble](#1-vue-densemble)
2. [Architecture G√©n√©rale](#2-architecture-g√©n√©rale)
3. [Flow de Donn√©es Complet](#3-flow-de-donn√©es-complet)
4. [Modules et Composants](#4-modules-et-composants)
5. [Base de Donn√©es](#5-base-de-donn√©es)
6. [Interactions entre Fichiers](#6-interactions-entre-fichiers)
7. [Patterns de Code](#7-patterns-de-code)
8. [Configuration et Anti-D√©tection](#8-configuration-et-anti-d√©tection)
9. [Gestion d'Erreurs](#9-gestion-derreurs)
10. [Guide D√©veloppeur](#10-guide-d√©veloppeur)

---

## 1. Vue d'Ensemble

### Objectif du Projet

Le MCP Hub Crawler est un syst√®me automatis√© de collecte et d'enrichissement de m√©tadonn√©es pour les serveurs MCP (Model Context Protocol). Il scrape, enrichit, parse et stocke des informations depuis :

- üåê **mcp.so** - Marketplace officiel
- üõí **mcpmarket.ai** - Marketplace alternatif
- üêô **GitHub API** - M√©tadonn√©es des repositories
- üì¶ **npm Registry** - Informations des packages Node.js

### Technologies Utilis√©es

| Technologie | Usage | Fichiers Cl√©s |
|-------------|-------|---------------|
| **Python 3.10+** | Langage principal | Tout le code |
| **SQLAlchemy** | ORM pour SQLite | `src/database/models_normalized.py` |
| **Playwright** | Scraping web avec navigateur | `src/scrapers/base_scraper.py` |
| **aiohttp** | Requ√™tes HTTP async | `src/enrichers/*.py` |
| **TypeScript** | Analyse de base de donn√©es | `scripts/tools/analysis/analyze-database.ts` |
| **SQLite** | Base de donn√©es | `data/mcp_servers.db` |

### Chiffres Cl√©s

- **199 serveurs** MCP collect√©s
- **11 tables** SQL normalis√©es
- **4 sources** de donn√©es externes
- **3 phases** de traitement principales
- **10 scripts** pipeline actifs
- **27 outils** d'analyse et maintenance

---

## 2. Architecture G√©n√©rale

### Sch√©ma d'Architecture Haut Niveau

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        SOURCES EXTERNES                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  mcp.so  ‚îÇ  mcpmarket.ai  ‚îÇ  GitHub API  ‚îÇ  npm Registry       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ              ‚îÇ               ‚îÇ
     ‚ñº            ‚ñº              ‚ñº               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      COUCHE COLLECTE                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  BaseScraper (Playwright)  ‚îÇ  GitHubEnricher  ‚îÇ  NpmEnricher   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      COUCHE PARSING                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ReadmeParser  ‚îÇ  ToolsParser  ‚îÇ  ParametersParser             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   COUCHE PERSISTANCE                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           SQLAlchemy Models + SQLite Database                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pipeline Principal

Le fichier **`scripts/pipeline/scrape_full_pipeline.py`** (365 lignes) orchestre tout :

```
Phase 1: Nettoyage DB
    ‚Üì
Phase 2: Collecte URLs depuis mcp.so
    ‚Üì
Phase 3: Scraping + Enrichissement par serveur
    ‚îú‚îÄ Scrape page serveur (mcp.so)
    ‚îú‚îÄ Enrichissement GitHub (si URL GitHub trouv√©e)
    ‚îú‚îÄ Enrichissement npm (si package npm trouv√©)
    ‚îú‚îÄ Parsing README (extraction config + env vars)
    ‚îî‚îÄ Sauvegarde en base de donn√©es
    ‚Üì
Phase 4: Statistiques et rapports
```

---

## 3. Flow de Donn√©es Complet

### Voyage d'une Donn√©e : De la Source √† la Base

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 1: COLLECTE URLS                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
mcp.so/servers
    ‚Üì [Playwright scraping avec pagination]
Liste d'URLs: ['mcp.so/servers/gitlab', 'mcp.so/servers/brave-search', ...]

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 2: SCRAPING SERVEUR                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
mcp.so/servers/gitlab
    ‚Üì [Playwright extraction]
Donn√©es brutes: {
    name: "GitLab MCP Server",
    slug: "gitlab",
    description: "Interact with GitLab API",
    github_url: "https://github.com/modelcontextprotocol/servers/tree/main/src/gitlab",
    npm_url: "@modelcontextprotocol/server-gitlab",
    tags: ["version-control", "api"]
}

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 3: ENRICHISSEMENT GITHUB                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
github_url
    ‚Üì [GitHub API v3 - 7 requ√™tes parall√®les]
    ‚îú‚îÄ GET /repos/{owner}/{repo} ‚Üí stars, forks, watchers, topics
    ‚îú‚îÄ GET /repos/{owner}/{repo}/readme ‚Üí README.md (base64)
    ‚îú‚îÄ GET /repos/{owner}/{repo}/languages ‚Üí {Python: 50%, JS: 30%}
    ‚îú‚îÄ GET /repos/{owner}/{repo}/contributors ‚Üí Top 10
    ‚îú‚îÄ GET /repos/{owner}/{repo}/releases/latest ‚Üí v1.2.3
    ‚îú‚îÄ GET /repos/{owner}/{repo}/commits?since=30d ‚Üí 45 commits
    ‚îî‚îÄ GET /repos/{owner}/{repo}/community/profile ‚Üí README‚úÖ, LICENSE‚úÖ
    ‚Üì
Donn√©es enrichies: {
    github_stars: 1234,
    github_forks: 56,
    primary_language: "Python",
    github_topics: ["mcp", "gitlab", "api"],
    github_health_score: 85,
    readme_content: "# GitLab MCP Server\n\n...",
    ...
}

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 4: ENRICHISSEMENT NPM                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
npm_package_name
    ‚Üì [npm Registry API]
    ‚îú‚îÄ GET /{package} ‚Üí version, license, repository
    ‚îî‚îÄ GET /api/npmjs.org/downloads/point/last-week/{pkg} ‚Üí 1250/week
    ‚Üì
Donn√©es npm: {
    npm_version: "1.0.3",
    npm_downloads_weekly: 1250,
    npm_license: "MIT",
    ...
}

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 5: PARSING README                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
readme_content
    ‚Üì [ReadmeParser.parse_all()]
    ‚îú‚îÄ extract_installation_config()
    ‚îÇ  ‚îú‚îÄ Pattern 1: JSON config blocks ‚Üí {"command": "npx", "args": [...]}
    ‚îÇ  ‚îú‚îÄ Pattern 2: git clone + install
    ‚îÇ  ‚îú‚îÄ Pattern 3: npx/npm commands
    ‚îÇ  ‚îî‚îÄ Pattern 4: docker run
    ‚îî‚îÄ extract_environment_variables()
       ‚îî‚îÄ GITLAB_TOKEN, GITLAB_URL, etc.
    ‚Üì
Config extracted: {
    installation_config: {
        type: "npm",
        command: "npx",
        args: ["-y", "@modelcontextprotocol/server-gitlab"],
        runtime: "node"
    },
    env_required: ["GITLAB_TOKEN", "GITLAB_URL"],
    env_descriptions: {
        "GITLAB_TOKEN": "GitLab personal access token",
        ...
    }
}

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 6: EXTRACTION TOOLS                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
readme_content
    ‚Üì [ToolsParser.parse_tools()]
    ‚îú‚îÄ Strategy 1: Extraction section "Available Tools"
    ‚îÇ  ‚îî‚îÄ Pattern: ### **tool_name** - description
    ‚îú‚îÄ Strategy 2: JSON schema code blocks
    ‚îî‚îÄ Strategy 3: Headings pattern matching
    ‚Üì
Tools extracted: [
    {
        name: "get_repository",
        display_name: "Get Repository",
        description: "Fetches information about a GitLab repository",
        input_schema: {...}
    },
    {
        name: "create_merge_request",
        ...
    }
]

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 7: EXTRACTION PARAMETERS                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
tool_documentation
    ‚Üì [ParametersParser.parse_parameters()]
    ‚îú‚îÄ Pattern 1: - `name` (type, optional): description
    ‚îú‚îÄ Pattern 2: - `name` - description (required)
    ‚îî‚îÄ Pattern 3: **Arguments:** list
    ‚Üì
Parameters extracted: [
    {
        name: "project_id",
        type: "string",
        description: "GitLab project ID",
        required: true
    },
    {
        name: "branch",
        type: "string",
        description: "Branch name",
        required: false,
        default: "main"
    }
]

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ √âTAPE 8: SAUVEGARDE DATABASE                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
All parsed data
    ‚Üì [save_enriched_server() - Transaction]
    ‚îú‚îÄ INSERT INTO servers (...) ‚Üí server_id
    ‚îú‚îÄ INSERT INTO markdown_content (...) ‚Üí liens vers server_id
    ‚îú‚îÄ INSERT INTO github_info (...) ‚Üí server_id FK
    ‚îú‚îÄ INSERT INTO npm_info (...) ‚Üí server_id FK
    ‚îú‚îÄ INSERT INTO mcp_config_npm (...) ‚Üí server_id FK
    ‚îú‚îÄ INSERT INTO tags + server_tags (M2M)
    ‚îú‚îÄ INSERT INTO tools (...) ‚Üí tool_id
    ‚îî‚îÄ INSERT INTO tool_parameters (...) ‚Üí tool_id FK
    ‚Üì
Database committed ‚úÖ
```

---

## 4. Modules et Composants

### 4.1 Scrapers (`src/scrapers/`)

#### **BaseScraper** (`base_scraper.py`, 287 lignes)

**R√¥le** : Classe de base pour le scraping avec Playwright, avec mesures anti-d√©tection.

**Utilisation** :
```python
async with BaseScraper(headless=True) as scraper:
    await scraper.navigate("https://mcp.so/servers")
    html = await scraper.get_html()
    links = await scraper.get_all_hrefs("a.server-link")
```

**M√©thodes Cl√©s** :

| M√©thode | Description | Retour |
|---------|-------------|--------|
| `start()` | Lance le navigateur Playwright | None |
| `navigate(url)` | Navigue vers une URL avec timeout | None |
| `get_html()` | R√©cup√®re le HTML de la page actuelle | str |
| `get_text(selector)` | Extrait le texte d'un s√©lecteur CSS | str |
| `get_all_hrefs(selector)` | Extrait tous les liens | List[str] |
| `wait_for_selector(selector)` | Attend l'apparition d'un √©l√©ment | None |
| `evaluate(script)` | Ex√©cute du JavaScript | Any |
| `close()` | Ferme le navigateur | None |

**Anti-D√©tection** :
- User-agents al√©atoires (5 variants)
- Viewports personnalis√©s
- Script d'init cachant `navigator.webdriver`
- Flags: `--disable-blink-features=AutomationControlled`

---

### 4.2 Enrichers (`src/enrichers/`)

#### **GitHubEnricher** (`github_enricher.py`, 578 lignes)

**R√¥le** : Enrichissement via GitHub REST API v3.

**Authentification** : Utilise `GITHUB_TOKEN` depuis `config/.env`
**Rate Limit** : 5000 req/heure (authentifi√©) vs 60/heure (non-auth)

**M√©thodes API** :

| M√©thode | Endpoint GitHub | Donn√©es R√©cup√©r√©es |
|---------|----------------|-------------------|
| `fetch_repository_info()` | `/repos/{owner}/{repo}` | stars, forks, watchers, topics, language, created_at, updated_at, archived |
| `fetch_readme()` | `/repos/{owner}/{repo}/readme` | README.md d√©cod√© (base64) |
| `fetch_languages()` | `/repos/{owner}/{repo}/languages` | Distribution langage {Python: 50000, JS: 30000} |
| `fetch_contributors()` | `/repos/{owner}/{repo}/contributors` | Top 10 contributeurs |
| `fetch_latest_release()` | `/repos/{owner}/{repo}/releases/latest` | Version, release notes, date |
| `fetch_commits_activity()` | `/repos/{owner}/{repo}/commits?since=30d` | Nombre de commits derniers 30j |
| `fetch_community_files()` | `/repos/{owner}/{repo}/community/profile` | Pr√©sence README, LICENSE, CONTRIBUTING, CODE_OF_CONDUCT |

**Health Score** (0-100) :
```python
def calculate_health_score(repo_info) -> int:
    score = 0
    score += min(repo_info['stars'] / 10, 20)      # Max 20 pts
    score += min(repo_info['commit_frequency'] * 2, 20)  # Max 20 pts
    score += repo_info['community_files_count'] * 5    # Max 20 pts (4 files)
    score += min(repo_info['contributors_count'] * 3, 15)  # Max 15 pts
    score += 10 if repo_info['has_recent_release'] else 0  # 10 pts
    score += 10 if not repo_info['archived'] else 0       # 10 pts
    return min(int(score), 100)
```

**Utilisation** :
```python
async with GitHubEnricher(token=GITHUB_TOKEN) as enricher:
    data = await enricher.fetch_comprehensive_info("modelcontextprotocol", "servers")
    # Retourne: dict avec 20+ champs
```

#### **NpmEnricher** (`npm_enricher.py`, 245 lignes)

**R√¥le** : Enrichissement via npm Registry API.

**Endpoints** :
- `https://registry.npmjs.org/{package}` - M√©tadonn√©es package
- `https://api.npmjs.org/downloads/point/last-week/{package}` - Statistiques t√©l√©chargement

**Donn√©es R√©cup√©r√©es** :
- Version actuelle
- License (SPDX)
- Repository URL (GitHub g√©n√©ralement)
- T√©l√©chargements hebdo/mensuels
- Date derni√®re publication
- Homepage

**Utilisation** :
```python
npm_enricher = NpmEnricher()
data = await npm_enricher.fetch_package_info("@modelcontextprotocol/server-gitlab")
# Retourne: {npm_version, npm_downloads_weekly, npm_license, ...}
```

---

### 4.3 Parsers (`src/parsers/`)

#### **ReadmeParser** (`readme_parser.py`, 496 lignes)

**R√¥le** : Extraction de la configuration d'installation et des variables d'environnement depuis README.md.

**M√©thodes Principales** :

| M√©thode | Priorit√© | Pattern Recherch√© | Exemple Output |
|---------|----------|-------------------|----------------|
| `_extract_json_config_blocks()` | Phase 1 | `{"mcpServers": {...}}` | Config JSON complet |
| `_extract_git_clone_install()` | Phase 2 | `git clone URL` + `npm install` | {type: "git", url, install_cmd} |
| `_extract_npm_config()` | Phase 3 | `npx @org/package` | {command: "npx", args: [...]} |
| `_extract_python_config()` | Phase 3 | `pip install package` | {command: "pip", args: [...]} |
| `_extract_docker_config()` | Phase 5 | `docker run -p 8080 image` | {image, ports, volumes} |
| `extract_environment_variables()` | Sec. | Section "Environment" | {env_vars: [], env_desc: {}} |

**Patterns de Configuration Support√©s** :

1. **JSON Config Blocks** (Priorit√© 1)
```json
{
  "mcpServers": {
    "gitlab": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-gitlab"],
      "env": {
        "GITLAB_TOKEN": "your-token"
      }
    }
  }
}
```

2. **Git Clone + Install** (Priorit√© 2)
```bash
git clone https://github.com/org/repo
cd repo
npm install
```

3. **NPM Direct** (Priorit√© 3)
```bash
npx -y @modelcontextprotocol/server-gitlab
```

4. **Python/pip** (Priorit√© 3)
```bash
pip install mcp-server-gitlab
python -m mcp_server_gitlab
```

5. **Docker** (Priorit√© 5)
```bash
docker run -p 8080:8080 org/mcp-gitlab
```

**Extraction Variables d'Environnement** :
```python
# Recherche sections avec keywords: "environment", "configuration", "env vars"
# Patterns d√©tect√©s:
# - VARIABLE_NAME: description
# - VARIABLE_NAME (required): description
# - export VARIABLE_NAME=value
```

#### **ToolsParser** (`tools_parser.py`, 370 lignes)

**R√¥le** : Extraction des outils (tools) depuis README.

**Strat√©gies Multi-Patterns** :

**Strat√©gie 1** : Section "Available Tools"
```markdown
## Available Tools

### **get_repository**
Fetches information about a GitLab repository.

### **create_merge_request**
Creates a new merge request.
```

**Strat√©gie 2** : JSON Schema
```json
{
  "tools": [
    {
      "name": "get_repository",
      "description": "Fetches repo info",
      "inputSchema": {...}
    }
  ]
}
```

**Strat√©gie 3** : Headings Pattern
```markdown
### `get_repository` / `list_branches`
```

**Strat√©gie 4** : Markdown Lists
```markdown
- `get_repository` - Fetches repository information
- `create_merge_request` - Creates a merge request
```

**Strat√©gie 5** : Tables
```markdown
| Tool | Description |
|------|-------------|
| `get_repository` | Fetches repo info |
```

**Output** :
```python
[
    {
        "name": "get_repository",
        "display_name": "Get Repository",
        "description": "Fetches information about a GitLab repository",
        "input_schema": {...}  # Si trouv√© dans JSON
    }
]
```

#### **ParametersParser** (`parameters_parser.py`, 348 lignes)

**R√¥le** : Extraction des param√®tres de chaque tool.

**Patterns Support√©s** :

**Pattern 1 - D√©taill√©** (playwright-mcp style)
```markdown
- `project_id` (string, required): GitLab project ID
- `branch` (string, optional): Branch name (default: main)
```

**Pattern 2 - Simple** (jina-mcp style)
```markdown
- `url` - URL to scrape (required)
- `format` - Output format: markdown or html (optional)
```

**Pattern 3 - Arguments Section**
```markdown
**Arguments:**
- project_id: Project identifier
- branch: Git branch
```

**Pattern 4 - JSON Examples**
```json
{
  "project_id": "123",
  "branch": "main"
}
```
‚Üí Inf√®re les types depuis les valeurs

**Output** :
```python
[
    {
        "name": "project_id",
        "type": "string",
        "description": "GitLab project ID",
        "required": True,
        "default": None,
        "example": "123"
    }
]
```

---

### 4.4 Database Models (`src/database/models_normalized.py`, 685 lignes)

Voir section [5. Base de Donn√©es](#5-base-de-donn√©es) pour le sch√©ma complet.

---

### 4.5 Scripts Pipeline (`scripts/pipeline/`)

#### **scrape_full_pipeline.py** (365 lignes)

**Le chef d'orchestre** - Coordonne toutes les phases.

**Structure** :
```python
async def main():
    # Phase 1: Clean database
    clean_database()

    # Phase 2: Collect server URLs from mcp.so
    urls = await scrape_server_list()

    # Phase 3: Process each server
    for url in urls:
        # 3a. Scrape basic info
        data = await scrape_single_server(scraper, url)

        # 3b. Enrich with GitHub
        if data['github_url']:
            github_data = await github_enricher.fetch_comprehensive_info(...)

        # 3c. Enrich with npm
        if data['npm_url']:
            npm_data = await npm_enricher.fetch_package_info(...)

        # 3d. Parse README config
        if github_data and 'readme' in github_data:
            parser = ReadmeParser(github_data['readme']['content'])
            config_data = parser.parse_all()

        # 3e. Save to database
        save_enriched_server(session, data, github_data, npm_data, ...)

    # Phase 4: Display stats
    print_statistics(stats)
```

**Fonction Cl√©** : `save_enriched_server()`
```python
def save_enriched_server(session, data, github_data, npm_data, readme, config, tags_map):
    """
    Sauvegarde un serveur et toutes ses donn√©es associ√©es.

    Transaction atomique : COMMIT si succ√®s, ROLLBACK si erreur.

    Tables modifi√©es:
    - servers (INSERT)
    - markdown_content (INSERT si readme fourni)
    - github_info (INSERT si github_data fourni)
    - npm_info (INSERT si npm_data fourni)
    - mcp_config_npm (INSERT si config fourni)
    - tags + server_tags (INSERT many-to-many)
    """
    # 1. Create server
    server = Server(id=str(uuid4()), slug=data['slug'], name=data['name'], ...)
    session.add(server)
    session.flush()  # Get server.id

    # 2. Save markdown content
    if readme:
        content = MarkdownContent(
            id=str(uuid4()),
            server_id=server.id,
            content_type='readme',
            content=readme['content'],
            ...
        )
        session.add(content)

    # 3. Save GitHub info
    if github_data:
        github = GithubInfo(
            id=str(uuid4()),
            server_id=server.id,
            github_url=github_data['github_url'],
            github_stars=github_data['stars'],
            ...
        )
        session.add(github)

    # 4. Save npm info (similaire)
    # 5. Save mcp_config_npm (similaire)
    # 6. Save tags (many-to-many)

    return True
```

#### **scrape_mcp_so.py** (365 lignes)

**Scraping du marketplace mcp.so**.

**Fonctions** :
- `scrape_server_list()` : Collecte URLs via pagination
- `scrape_single_server(url)` : Scrape page serveur individuelle

**Pattern de Pagination** :
```python
async def scrape_server_list(scraper, max_urls=100):
    urls = []
    page = 1

    while len(urls) < max_urls:
        await scraper.navigate(f"https://mcp.so/servers?page={page}")
        links = await scraper.get_all_hrefs("a.server-card")

        if not links:  # No more results
            break

        urls.extend(links)
        page += 1
        await asyncio.sleep(random.uniform(3, 7))  # Anti-detection

    return urls[:max_urls]
```

#### **enrich_*.py** (6 fichiers d'enrichment)

Scripts sp√©cialis√©s pour des serveurs sp√©cifiques :
- `enrich_github_info.py` : Enrichissement GitHub batch
- `enrich_flomo.py` : Enrichissement serveur Flomo
- `enrich_perplexity.py` : Enrichissement serveur Perplexity
- `enrich_minimax.py` : Enrichissement serveur Minimax
- `enrich_serper.py` : Enrichissement serveur Serper

---

## 5. Base de Donn√©es

### Sch√©ma Complet (11 Tables)

```sql
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          SERVERS (Core)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ id (PK) ‚îÇ slug (UQ) ‚îÇ name ‚îÇ display_name ‚îÇ tagline            ‚îÇ
‚îÇ short_description ‚îÇ status ‚îÇ verification_status ‚îÇ tools_count ‚îÇ
‚îÇ created_at ‚îÇ updated_at                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                                                 ‚îÇ
     ‚ñº                                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MARKDOWN_CONTENT      ‚îÇ              ‚îÇ     GITHUB_INFO         ‚îÇ
‚îÇ   (1-to-Many)           ‚îÇ              ‚îÇ     (1-to-1)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ server_id (FK)          ‚îÇ              ‚îÇ server_id (FK, UQ)      ‚îÇ
‚îÇ content_type (about/    ‚îÇ              ‚îÇ github_url              ‚îÇ
‚îÇ   readme/faq/tools)     ‚îÇ              ‚îÇ github_stars            ‚îÇ
‚îÇ content (TEXT)          ‚îÇ              ‚îÇ github_forks            ‚îÇ
‚îÇ word_count              ‚îÇ              ‚îÇ github_health_score     ‚îÇ
‚îÇ UQ(server_id,           ‚îÇ              ‚îÇ primary_language        ‚îÇ
‚îÇ    content_type)        ‚îÇ              ‚îÇ github_topics (JSON)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ last_synced_at          ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                                                 ‚îÇ
     ‚ñº                                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       NPM_INFO          ‚îÇ              ‚îÇ   MCP_CONFIG_NPM        ‚îÇ
‚îÇ       (1-to-1)          ‚îÇ              ‚îÇ   (1-to-1)              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ server_id (FK, UQ)      ‚îÇ              ‚îÇ server_id (FK, UQ)      ‚îÇ
‚îÇ npm_package             ‚îÇ              ‚îÇ command (npx/npm/node)  ‚îÇ
‚îÇ npm_version             ‚îÇ              ‚îÇ args (JSON array)       ‚îÇ
‚îÇ npm_downloads_weekly    ‚îÇ              ‚îÇ env_required (JSON)     ‚îÇ
‚îÇ npm_license             ‚îÇ              ‚îÇ env_descriptions (JSON) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ runtime (node/python)   ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ        TOOLS            ‚îÇ
                ‚îÇ      (1-to-Many)        ‚îÇ
                ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                ‚îÇ server_id (FK)          ‚îÇ
                ‚îÇ name (snake_case)       ‚îÇ
                ‚îÇ display_name            ‚îÇ
                ‚îÇ description             ‚îÇ
                ‚îÇ input_schema (JSON)     ‚îÇ
                ‚îÇ is_dangerous            ‚îÇ
                ‚îÇ requires_auth           ‚îÇ
                ‚îÇ UQ(server_id, name)     ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   TOOL_PARAMETERS       ‚îÇ
           ‚îÇ   (Many per Tool)       ‚îÇ
           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
           ‚îÇ tool_id (FK)            ‚îÇ
           ‚îÇ name                    ‚îÇ
           ‚îÇ type (string/int/etc.)  ‚îÇ
           ‚îÇ description             ‚îÇ
           ‚îÇ required (boolean)      ‚îÇ
           ‚îÇ default_value (JSON)    ‚îÇ
           ‚îÇ example_value (JSON)    ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Many-to-Many ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                                              ‚îÇ
     ‚ñº                                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CATEGORIES   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇSERVER_CATEGORIES‚îÇ    ‚îÇ    TAGS      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ slug (UQ)    ‚îÇ    ‚îÇ server_id (FK) ‚îÇ    ‚îÇ slug (UQ)    ‚îÇ
‚îÇ name (UQ)    ‚îÇ    ‚îÇ category_id(FK)‚îÇ    ‚îÇ name (UQ)    ‚îÇ
‚îÇ server_count ‚îÇ    ‚îÇ PK(server_id,  ‚îÇ    ‚îÇ server_count ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    category_id)‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚ñ≤
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  SERVER_TAGS   ‚îÇ
                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    ‚îÇ server_id (FK) ‚îÇ
                    ‚îÇ tag_id (FK)    ‚îÇ
                    ‚îÇ PK(server_id,  ‚îÇ
                    ‚îÇ    tag_id)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Tables D√©taill√©es

#### **servers**
```sql
CREATE TABLE servers (
    id VARCHAR(36) PRIMARY KEY,
    slug VARCHAR(255) UNIQUE NOT NULL,
    name TEXT NOT NULL,
    display_name TEXT NOT NULL,
    tagline TEXT,
    short_description TEXT,
    status VARCHAR(20) CHECK(status IN ('approved', 'pending', 'rejected')) DEFAULT 'approved',
    verification_status VARCHAR(20) CHECK(verification_status IN ('verified', 'unverified')) DEFAULT 'unverified',
    creator_name TEXT,
    tools_count INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_servers_slug ON servers(slug);
CREATE INDEX idx_servers_status ON servers(status);
CREATE INDEX idx_servers_updated_at ON servers(updated_at DESC);
```

#### **markdown_content**
```sql
CREATE TABLE markdown_content (
    id VARCHAR(36) PRIMARY KEY,
    server_id VARCHAR(36) NOT NULL,
    content_type VARCHAR(20) CHECK(content_type IN ('about', 'readme', 'faq', 'tools')) NOT NULL,
    content TEXT NOT NULL,
    word_count INTEGER,
    estimated_reading_time_minutes INTEGER,
    extracted_from TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (server_id) REFERENCES servers(id) ON DELETE CASCADE,
    UNIQUE (server_id, content_type)
);
```

#### **github_info**
```sql
CREATE TABLE github_info (
    id VARCHAR(36) PRIMARY KEY,
    server_id VARCHAR(36) UNIQUE NOT NULL,
    github_url TEXT NOT NULL,
    github_owner TEXT,
    github_repo TEXT,
    github_stars INTEGER DEFAULT 0,
    github_forks INTEGER DEFAULT 0,
    github_watchers INTEGER DEFAULT 0,
    github_last_commit DATETIME,
    commit_frequency INTEGER,
    primary_language TEXT,
    languages TEXT,  -- JSON: {Python: 50000, JS: 30000}
    github_topics TEXT,  -- JSON array
    github_health_score INTEGER,
    has_readme INTEGER DEFAULT 0,
    has_license INTEGER DEFAULT 0,
    has_contributing INTEGER DEFAULT 0,
    github_created_at DATETIME,
    last_synced_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (server_id) REFERENCES servers(id) ON DELETE CASCADE
);
```

#### **tools**
```sql
CREATE TABLE tools (
    id VARCHAR(36) PRIMARY KEY,
    server_id VARCHAR(36) NOT NULL,
    name TEXT NOT NULL,
    display_name TEXT,
    description TEXT,
    input_schema TEXT,  -- JSON Schema
    example_usage TEXT,
    example_response TEXT,
    is_dangerous INTEGER DEFAULT 0,
    requires_auth INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (server_id) REFERENCES servers(id) ON DELETE CASCADE,
    UNIQUE (server_id, name)
);
```

#### **tool_parameters**
```sql
CREATE TABLE tool_parameters (
    id VARCHAR(36) PRIMARY KEY,
    tool_id VARCHAR(36) NOT NULL,
    name TEXT NOT NULL,
    type TEXT,  -- string, integer, array, object
    description TEXT,
    required INTEGER DEFAULT 0,
    default_value TEXT,  -- JSON string
    example_value TEXT,  -- JSON string
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (tool_id) REFERENCES tools(id) ON DELETE CASCADE
);
```

---

## 6. Interactions entre Fichiers

### Graphe d'Import

```
scrape_full_pipeline.py (365L)
‚îú‚îÄ IMPORTS
‚îÇ  ‚îú‚îÄ src.database.models_normalized
‚îÇ  ‚îÇ  ‚îî‚îÄ Server, GithubInfo, NpmInfo, MarkdownContent, McpConfigNpm, Tag, ServerTag
‚îÇ  ‚îú‚îÄ src.enrichers.github_enricher
‚îÇ  ‚îÇ  ‚îî‚îÄ GitHubEnricher (async class)
‚îÇ  ‚îú‚îÄ src.enrichers.npm_enricher
‚îÇ  ‚îÇ  ‚îî‚îÄ NpmEnricher (async class)
‚îÇ  ‚îú‚îÄ src.parsers.readme_parser
‚îÇ  ‚îÇ  ‚îî‚îÄ ReadmeParser (class)
‚îÇ  ‚îú‚îÄ scripts.pipeline.scrape_mcp_so
‚îÇ  ‚îÇ  ‚îî‚îÄ scrape_server_list(), scrape_single_server()
‚îÇ  ‚îî‚îÄ scripts.config
‚îÇ     ‚îî‚îÄ PHASE1_CONFIG, PHASE2_CONFIG
‚îÇ
‚îî‚îÄ CALLS
   ‚îú‚îÄ clean_database() ‚Üí scripts/clean_database.py
   ‚îú‚îÄ scrape_server_list() ‚Üí scrape_mcp_so.py
   ‚îú‚îÄ scrape_single_server() ‚Üí scrape_mcp_so.py
   ‚îú‚îÄ github_enricher.fetch_comprehensive_info() ‚Üí GitHubEnricher
   ‚îú‚îÄ npm_enricher.fetch_package_info() ‚Üí NpmEnricher
   ‚îú‚îÄ readme_parser.parse_all() ‚Üí ReadmeParser
   ‚îî‚îÄ save_enriched_server() ‚Üí INSERTS en base

scrape_mcp_so.py (365L)
‚îú‚îÄ IMPORTS
‚îÇ  ‚îú‚îÄ src.scrapers.base_scraper
‚îÇ  ‚îÇ  ‚îî‚îÄ BaseScraper (async class)
‚îÇ  ‚îú‚îÄ src.parsers.tools_parser
‚îÇ  ‚îÇ  ‚îî‚îÄ ToolsParser (class)
‚îÇ  ‚îú‚îÄ src.parsers.parameters_parser
‚îÇ  ‚îÇ  ‚îî‚îÄ ParametersParser (class)
‚îÇ  ‚îî‚îÄ src.database.models_normalized
‚îÇ     ‚îî‚îÄ Server, Tool, ToolParameter
‚îÇ
‚îî‚îÄ USES
   ‚îú‚îÄ BaseScraper.navigate() ‚Üí Playwright page.goto()
   ‚îú‚îÄ BaseScraper.get_all_hrefs() ‚Üí page.query_selector_all()
   ‚îî‚îÄ ToolsParser.parse_tools() ‚Üí Parse README

GitHubEnricher (src/enrichers/github_enricher.py, 578L)
‚îú‚îÄ IMPORTS
‚îÇ  ‚îú‚îÄ aiohttp (async HTTP client)
‚îÇ  ‚îú‚îÄ dotenv (charge config/.env)
‚îÇ  ‚îî‚îÄ pathlib.Path
‚îÇ
‚îî‚îÄ USES
   ‚îú‚îÄ aiohttp.ClientSession.get() ‚Üí GitHub API
   ‚îú‚îÄ X-RateLimit-* headers ‚Üí Rate limiting
   ‚îî‚îÄ base64.b64decode() ‚Üí D√©codage README

ReadmeParser (src/parsers/readme_parser.py, 496L)
‚îú‚îÄ NO EXTERNAL IMPORTS (pure Python)
‚îî‚îÄ USES
   ‚îú‚îÄ re.search(), re.findall() ‚Üí Pattern matching
   ‚îú‚îÄ json.loads() ‚Üí Parse JSON configs
   ‚îî‚îÄ Markdown heading detection

ToolsParser (src/parsers/tools_parser.py, 370L)
‚îú‚îÄ IMPORTS
‚îÇ  ‚îî‚îÄ re (regex)
‚îî‚îÄ USES
   ‚îú‚îÄ Multiple regex patterns
   ‚îî‚îÄ 3 parsing strategies (section/JSON/headings)

SQLAlchemy Models (src/database/models_normalized.py, 685L)
‚îú‚îÄ IMPORTS
‚îÇ  ‚îú‚îÄ sqlalchemy (ORM)
‚îÇ  ‚îî‚îÄ datetime, uuid
‚îî‚îÄ DEFINES
   ‚îú‚îÄ Base = declarative_base()
   ‚îî‚îÄ 11 table classes
```

### Flux d'Appel D√©taill√©

```
main() @ scrape_full_pipeline.py
‚îÇ
‚îú‚îÄ Phase 1: clean_database()
‚îÇ  ‚îî‚îÄ Bash call: python scripts/clean_database.py
‚îÇ
‚îú‚îÄ Phase 2: scrape_server_list()
‚îÇ  ‚îî‚îÄ scrape_mcp_so.scrape_server_list()
‚îÇ     ‚îî‚îÄ BaseScraper.navigate()
‚îÇ        ‚îî‚îÄ Playwright async_api.chromium.launch()
‚îÇ           ‚îî‚îÄ page.goto("https://mcp.so/servers")
‚îÇ              ‚îî‚îÄ page.query_selector_all("a.server-card")
‚îÇ
‚îî‚îÄ Phase 3: Loop over URLs
   ‚îú‚îÄ scrape_single_server(url)
   ‚îÇ  ‚îî‚îÄ scrape_mcp_so.scrape_single_server()
   ‚îÇ     ‚îî‚îÄ BaseScraper.navigate(url)
   ‚îÇ        ‚îî‚îÄ Extract: name, description, github_url, npm_url, tags
   ‚îÇ
   ‚îú‚îÄ IF github_url:
   ‚îÇ  ‚îî‚îÄ github_enricher.fetch_comprehensive_info(owner, repo)
   ‚îÇ     ‚îú‚îÄ fetch_repository_info()
   ‚îÇ     ‚îÇ  ‚îî‚îÄ aiohttp.get("https://api.github.com/repos/{owner}/{repo}")
   ‚îÇ     ‚îú‚îÄ fetch_readme()
   ‚îÇ     ‚îÇ  ‚îî‚îÄ aiohttp.get("/repos/{owner}/{repo}/readme")
   ‚îÇ     ‚îÇ     ‚îî‚îÄ base64.b64decode(content)
   ‚îÇ     ‚îú‚îÄ fetch_languages()
   ‚îÇ     ‚îú‚îÄ fetch_contributors()
   ‚îÇ     ‚îú‚îÄ fetch_latest_release()
   ‚îÇ     ‚îú‚îÄ fetch_commits_activity()
   ‚îÇ     ‚îî‚îÄ fetch_community_files()
   ‚îÇ
   ‚îú‚îÄ IF npm_url:
   ‚îÇ  ‚îî‚îÄ npm_enricher.fetch_package_info(package_name)
   ‚îÇ     ‚îú‚îÄ aiohttp.get("https://registry.npmjs.org/{package}")
   ‚îÇ     ‚îî‚îÄ aiohttp.get("https://api.npmjs.org/downloads/point/last-week/{pkg}")
   ‚îÇ
   ‚îú‚îÄ IF readme_content:
   ‚îÇ  ‚îî‚îÄ readme_parser.parse_all(readme_content)
   ‚îÇ     ‚îú‚îÄ extract_installation_config()
   ‚îÇ     ‚îÇ  ‚îú‚îÄ _extract_json_config_blocks()
   ‚îÇ     ‚îÇ  ‚îÇ  ‚îî‚îÄ json.loads(code_block)
   ‚îÇ     ‚îÇ  ‚îú‚îÄ _extract_npm_config()
   ‚îÇ     ‚îÇ  ‚îÇ  ‚îî‚îÄ re.search(r"npx\s+(.+)")
   ‚îÇ     ‚îÇ  ‚îî‚îÄ _extract_python_config()
   ‚îÇ     ‚îÇ     ‚îî‚îÄ re.search(r"pip install (.+)")
   ‚îÇ     ‚îî‚îÄ extract_environment_variables()
   ‚îÇ        ‚îî‚îÄ re.findall(r"([A-Z_]+):\s*(.+)")
   ‚îÇ
   ‚îî‚îÄ save_enriched_server(data, github_data, npm_data, config)
      ‚îî‚îÄ SQLAlchemy Transaction
         ‚îú‚îÄ session.add(Server(...))
         ‚îú‚îÄ session.flush() ‚Üí Get server_id
         ‚îú‚îÄ session.add(MarkdownContent(server_id=...))
         ‚îú‚îÄ session.add(GithubInfo(server_id=...))
         ‚îú‚îÄ session.add(NpmInfo(server_id=...))
         ‚îú‚îÄ session.add(McpConfigNpm(server_id=...))
         ‚îú‚îÄ FOR tag IN tags:
         ‚îÇ  ‚îî‚îÄ session.add(ServerTag(server_id, tag_id))
         ‚îî‚îÄ session.commit()  # Atomique
```

---

## 7. Patterns de Code

### 7.1 Async Context Managers

**Pattern** : Gestion automatique de ressources avec `async with`.

```python
# Enrichers
async with GitHubEnricher(token=GITHUB_TOKEN) as enricher:
    data = await enricher.fetch_repository_info(owner, repo)
    # Auto-fermeture de la session aiohttp

# Scrapers
async with BaseScraper(headless=True) as scraper:
    await scraper.navigate(url)
    html = await scraper.get_html()
    # Auto-fermeture du navigateur Playwright

# Impl√©mentation
class GitHubEnricher:
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
```

**Avantages** :
- Cleanup automatique (fermeture connexions)
- Gestion d'erreurs propre
- Code lisible

### 7.2 Multiple Strategy Parsing

**Pattern** : Tentatives successives avec fallbacks.

```python
class ToolsParser:
    def parse_tools(self, markdown):
        # Strategy 1: Extract "Tools" section
        tools = self._parse_tools_from_section(markdown)
        if tools:
            return tools

        # Strategy 2: Parse JSON code blocks
        tools = self._parse_tools_from_code_blocks(markdown)
        if tools:
            return tools

        # Strategy 3: Fallback - scan all headings
        tools = self._parse_tools_from_headings(markdown)
        return tools
```

**Avantages** :
- Robustesse face √† diff√©rents formats
- Couverture maximale des cas
- D√©gradation gracieuse

### 7.3 Transactional Database Operations

**Pattern** : Transactions atomiques avec rollback.

```python
def save_enriched_server(session, data, github_data, npm_data, ...):
    try:
        # 1. Create server
        server = Server(...)
        session.add(server)
        session.flush()  # Get ID without committing

        # 2. Create related records
        if github_data:
            github = GithubInfo(server_id=server.id, ...)
            session.add(github)

        # 3. Commit ALL or NOTHING
        session.commit()
        return True

    except Exception as e:
        session.rollback()  # Undo ALL changes
        logger.error(f"Failed: {e}")
        return False
```

**Avantages** :
- Coh√©rence des donn√©es garantie
- Pas d'√©tats partiels en base
- Facilite debugging

### 7.4 Rate Limiting with Headers

**Pattern** : Respect des limites API.

```python
class GitHubEnricher:
    async def _make_request(self, url):
        async with self.session.get(url) as response:
            # Check rate limit
            remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))

            if remaining < 10:
                wait_seconds = reset_time - time.time()
                logger.warning(f"Rate limit low. Waiting {wait_seconds}s")
                await asyncio.sleep(wait_seconds + 5)

            return await response.json()
```

**Avantages** :
- Pas de blocage API
- Scraping continu
- Respect des ToS

### 7.5 Anti-Detection Measures

**Pattern** : Comportement humain simul√©.

```python
def get_random_delay(min_delay, max_delay):
    base_delay = random.uniform(min_delay, max_delay)
    jitter = random.uniform(-0.2, 0.3)  # ¬±20-30% variation
    return max(0.5, base_delay + jitter)

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ...',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ...',
    # 5 variants
]

def get_random_user_agent():
    return random.choice(USER_AGENTS)

VIEWPORT_SIZES = [
    {'width': 1920, 'height': 1080},
    {'width': 1366, 'height': 768},
    # 5 variants
]

def get_random_viewport():
    return random.choice(VIEWPORT_SIZES)
```

**Utilis√© par** : BaseScraper, config.py

### 7.6 Exponential Backoff Retry

**Pattern** : Retry avec d√©lai exponentiel.

```python
def calculate_backoff_delay(attempt, base_delay, max_delay):
    """
    Exponential: delay = base * (2 ^ attempt)
    With jitter: ¬±20%
    """
    delay = base_delay * (2 ** attempt)
    delay = min(delay, max_delay)
    jitter = delay * random.uniform(-0.2, 0.2)
    return delay + jitter

# Usage
for attempt in range(MAX_RETRIES):
    try:
        result = await fetch_data(url)
        break
    except Exception as e:
        if attempt < MAX_RETRIES - 1:
            delay = calculate_backoff_delay(attempt, base=5.0, max=120.0)
            await asyncio.sleep(delay)
        else:
            raise
```

**Sc√©nario** :
- Attempt 0: 5s (base)
- Attempt 1: 10s (2√ó5)
- Attempt 2: 20s (4√ó5)
- Attempt 3: 40s (8√ó5)
- Attempt 4: 80s (16√ó5)
- Attempt 5: 120s (capped)

### 7.7 Lazy Loading with flush()

**Pattern** : Obtenir l'ID sans commit.

```python
# Create server
server = Server(id=str(uuid4()), ...)
session.add(server)
session.flush()  # INSERT + get auto-generated ID

# Use server.id immediately without committing
github_info = GithubInfo(server_id=server.id, ...)
session.add(github_info)

# Commit all at once
session.commit()
```

**Avantages** :
- Relations cr√©√©es avant commit
- Rollback possible si erreur ult√©rieure
- Performance (batch commit)

---

## 8. Configuration et Anti-D√©tection

### Configuration Centralis√©e (`scripts/config.py`)

**PHASE1_CONFIG** (Collecte URLs)
```python
{
    'TARGET_URLS': 100,                # Nombre d'URLs √† collecter
    'MAX_PAGES': 50,                   # Pages max de pagination
    'DELAY_BETWEEN_PAGES_MIN': 3.0,    # D√©lai min entre pages (s)
    'DELAY_BETWEEN_PAGES_MAX': 7.0,    # D√©lai max entre pages (s)
    'INITIAL_PAGE_WAIT': 5.0,          # Attente initiale JS render
    'PAGE_LOAD_WAIT': 3.0,             # Attente chargement page
    'MAX_PAGE_RETRIES': 3,             # Retry si √©chec
    'RETRY_DELAY_BASE': 5.0,           # Base d√©lai retry
    'RETRY_DELAY_MAX': 60.0,           # Max d√©lai retry
    'BATCH_SIZE': 50,                  # Taille batch DB
    'EMPTY_PAGE_THRESHOLD': 3,         # Pages vides avant stop
    'HEADLESS': True,                  # Mode headless
    'TIMEOUT': 30000,                  # Timeout page (ms)
    'ROTATE_USER_AGENT': True,         # Rotation user-agent
    'RANDOM_VIEWPORT': True,           # Viewport al√©atoire
    'SIMULATE_HUMAN': True,            # Comportement humain
}
```

**PHASE2_CONFIG** (Scraping serveurs)
```python
{
    'BATCH_SIZE': 10,
    'MAX_RETRIES': 3,
    'DELAY_BETWEEN_URLS_MIN': 2.0,
    'DELAY_BETWEEN_URLS_MAX': 5.0,
    'DELAY_BETWEEN_BATCHES_MIN': 10.0,
    'DELAY_BETWEEN_BATCHES_MAX': 20.0,
    'INITIAL_PAGE_WAIT': 4.0,
    'RETRY_DELAY_BASE': 10.0,
    'RETRY_DELAY_MAX': 120.0,
    'HEADLESS': True,
    'TIMEOUT': 30000,
    'PARALLEL_WORKERS': 1,             # 1 pour s√©curit√©
    'ROTATE_USER_AGENT': True,
    'RANDOM_VIEWPORT': True,
    'SIMULATE_HUMAN': True,
}
```

**Sc√©narios Pr√©d√©finis** :
- `dev` : 10 serveurs, rapide (test)
- `production_100` : 100 serveurs, √©quilibr√©
- `production_1000` : 1000 serveurs, mod√©r√©
- `production_10000` : 10000 serveurs, agressif mais safe

```python
phase1, phase2 = apply_scenario('production_100')
```

### Mesures Anti-D√©tection

**1. Rotation User-Agent** :
```python
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 ...',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 ... Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 ...',
]
```

**2. Viewports Al√©atoires** :
```python
VIEWPORT_SIZES = [
    {'width': 1920, 'height': 1080},  # Full HD
    {'width': 1366, 'height': 768},   # Laptop commun
    {'width': 1536, 'height': 864},   # Laptop
    {'width': 1440, 'height': 900},   # MacBook
    {'width': 1280, 'height': 720},   # HD
]
```

**3. D√©lais Al√©atoires** :
```python
def get_random_delay(min_delay, max_delay):
    base_delay = random.uniform(min_delay, max_delay)
    jitter = random.uniform(-0.2, 0.3)  # Variation humaine
    return max(0.5, base_delay + jitter)
```

**4. Playwright Hardening** :
```python
browser = await playwright.chromium.launch(
    headless=True,
    args=[
        '--disable-blink-features=AutomationControlled',
        '--no-sandbox',
        '--disable-dev-shm-usage',
    ]
)

# Init script
await context.add_init_script("""
    Object.defineProperty(navigator, 'webdriver', {
        get: () => undefined
    });
""")
```

**5. Rate Limiting GitHub** :
```python
if remaining < 10:
    wait_seconds = reset_time - time.time()
    logger.warning(f"Rate limit: {remaining}. Waiting {wait_seconds}s")
    await asyncio.sleep(wait_seconds + 5)
```

---

## 9. Gestion d'Erreurs

### Strat√©gie Globale

**Principe** : **D√©gradation gracieuse** - Les √©checs partiels ne bloquent pas le pipeline.

### Niveaux d'Erreur

#### **Niveau 1 : Database (CRITICAL)**

Transaction atomique - **ROLLBACK si erreur**.

```python
try:
    save_enriched_server(session, data, github_data, ...)
    session.commit()  # TOUT ou RIEN
    stats['saved'] += 1
except Exception as e:
    session.rollback()  # Annule TOUTES les modifications
    stats['errors'] += 1
    logger.error(f"DB error: {e}")
    continue  # Skip au serveur suivant
```

**Comportement** : Si la sauvegarde d'un serveur √©choue, le serveur est ignor√© mais le pipeline continue.

#### **Niveau 2 : Enrichment (OPTIONAL)**

√âchec non-bloquant - **Continuer avec None**.

```python
# GitHub enrichment
github_data = None
try:
    if data.get('github_url'):
        github_data = await github_enricher.fetch_comprehensive_info(owner, repo)
except Exception as e:
    logger.warning(f"GitHub enrichment failed: {e}")
    github_data = None  # Continue sans donn√©es GitHub

# npm enrichment
npm_data = None
try:
    if data.get('npm_url'):
        npm_data = await npm_enricher.fetch_package_info(package_name)
except Exception as e:
    logger.warning(f"npm enrichment failed: {e}")
    npm_data = None  # Continue sans donn√©es npm
```

**Comportement** : Si GitHub/npm fail, le serveur est quand m√™me sauvegard√© avec les donn√©es disponibles.

#### **Niveau 3 : Parsing (ENHANCEMENT)**

√âchec non-bloquant - **Continuer sans config**.

```python
config_data = None
try:
    if readme_content:
        parser = ReadmeParser(readme_content['content'])
        config_data = parser.parse_all()
except Exception as e:
    logger.warning(f"README parsing failed: {e}")
    config_data = None  # Pas de config mais serveur sauvegard√©
```

**Comportement** : Le serveur est sauvegard√© m√™me sans config d'installation extraite.

#### **Niveau 4 : Scraping (RETRY + SKIP)**

Retry avec backoff, puis skip si √©chec.

```python
for attempt in range(MAX_RETRIES):
    try:
        data = await scrape_single_server(scraper, url)
        break  # Success
    except Exception as e:
        if attempt < MAX_RETRIES - 1:
            delay = calculate_backoff_delay(attempt, base=5.0, max=60.0)
            logger.warning(f"Retry {attempt+1}/{MAX_RETRIES} after {delay}s")
            await asyncio.sleep(delay)
        else:
            logger.error(f"Failed after {MAX_RETRIES} attempts: {e}")
            stats['errors'] += 1
            continue  # Skip ce serveur
```

**Comportement** : 3 tentatives avec d√©lai exponentiel, puis skip.

### Logging

**Niveaux** :
- `ERROR` : √âchecs critiques (DB, scraping apr√®s retries)
- `WARNING` : √âchecs non-bloquants (GitHub fail, parsing fail)
- `INFO` : Progression normale

**Exemple Log** :
```
[INFO] Processing server 45/100: gitlab
[INFO] GitHub enrichment: 1234 stars, health score 85
[WARNING] npm package not found: @modelcontextprotocol/server-gitlab
[INFO] README parsed: npx config extracted
[INFO] Saved server gitlab (7 tools, 23 parameters)
[INFO] Progress: 45/100 (45%), Saved: 44, Errors: 1
```

### Statistiques Finales

```python
stats = {
    'total': 100,
    'saved': 96,
    'errors': 4,
    'github_enriched': 92,
    'npm_enriched': 88,
    'config_extracted': 85,
    'tools_extracted': 78,
}

success_rate = (stats['saved'] / stats['total']) * 100
print(f"Success rate: {success_rate:.1f}%")
```

---

## 10. Guide D√©veloppeur

### 10.1 Setup Environnement

**1. Installation**
```bash
# Clone repo
git clone <repo-url>
cd "crawler MCPhub"

# Environnement virtuel Python
python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# D√©pendances Python
pip install -r config/requirements.txt

# D√©pendances Node.js (pour analyse TypeScript)
npm install --prefix config/
```

**2. Configuration**
```bash
# Copier template
cp config/.env.example config/.env

# √âditer config/.env
# GITHUB_TOKEN=ghp_your_token_here
# NPM_TOKEN=npm_optional_token
```

**3. V√©rification**
```bash
# Test imports
python -c "from src.database.models_normalized import Base; print('OK')"

# Test Playwright
playwright install chromium

# Test base de donn√©es
ls data/mcp_servers.db  # Doit exister (7.8 MB)
```

### 10.2 Ex√©cuter le Pipeline

**Pipeline complet** :
```bash
python scripts/pipeline/scrape_full_pipeline.py --max-servers 10
```

**Phases individuelles** :
```bash
# Phase 1: Collecte URLs
python scripts/pipeline/scrape_mcp_so.py

# Phase 2: Enrichissement GitHub
python scripts/pipeline/enrich_github_info.py

# Analyse database
node scripts/tools/analysis/analyze-database.js
```

**Avec sc√©narios** :
```python
# Dans le script
from scripts.config import apply_scenario

phase1, phase2 = apply_scenario('dev')  # 10 serveurs rapide
phase1, phase2 = apply_scenario('production_100')  # 100 serveurs
```

### 10.3 Ajouter un Nouveau Parser

**Exemple** : Parser pour extraire les prix depuis README.

**1. Cr√©er le fichier** `src/parsers/pricing_parser.py`

```python
import re
from typing import Optional, Dict

class PricingParser:
    """Extract pricing information from README markdown."""

    def __init__(self, markdown_content: str):
        self.content = markdown_content

    def extract_pricing(self) -> Optional[Dict]:
        """
        Extract pricing from README.

        Patterns:
        - ## Pricing
        - $X/month
        - Free tier: ...

        Returns:
            Dict with {tier, price, currency} or None
        """
        # Find "Pricing" section
        pricing_section = self._find_section(['pricing', 'cost', 'price'])
        if not pricing_section:
            return None

        # Extract price pattern: $X/month, ‚Ç¨X/mo, etc.
        pattern = r'([‚Ç¨$¬£])(\d+(?:\.\d{2})?)\s*/\s*(month|mo|year|yr)'
        match = re.search(pattern, pricing_section, re.IGNORECASE)

        if match:
            return {
                'currency': match.group(1),
                'amount': float(match.group(2)),
                'period': match.group(3).lower(),
                'tier': 'paid'
            }

        # Check for "free" mentions
        if re.search(r'\bfree\b', pricing_section, re.IGNORECASE):
            return {
                'currency': None,
                'amount': 0,
                'period': None,
                'tier': 'free'
            }

        return None

    def _find_section(self, keywords):
        """Find section by keywords (same as ReadmeParser)."""
        for keyword in keywords:
            pattern = rf'^#+\s*{keyword}.*?(?=^#+|\Z)'
            match = re.search(pattern, self.content, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            if match:
                return match.group(0)
        return None
```

**2. Ajouter table database** `src/database/models_normalized.py`

```python
class PricingInfo(Base):
    __tablename__ = 'pricing_info'

    id = Column(String(36), primary_key=True, default=lambda: str(uuid4()))
    server_id = Column(String(36), ForeignKey('servers.id', ondelete='CASCADE'), unique=True, nullable=False)
    tier = Column(String(20))  # 'free', 'paid', 'freemium'
    currency = Column(String(3))  # 'USD', 'EUR', 'GBP'
    amount = Column(Float)
    period = Column(String(10))  # 'month', 'year'
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationship
    server = relationship('Server', backref='pricing')
```

**3. Int√©grer dans pipeline** `scripts/pipeline/scrape_full_pipeline.py`

```python
from src.parsers.pricing_parser import PricingParser

# Dans la boucle de traitement
if readme_content:
    # Existing parsers
    readme_parser = ReadmeParser(readme_content['content'])
    config_data = readme_parser.parse_all()

    # NEW: Pricing parser
    pricing_parser = PricingParser(readme_content['content'])
    pricing_data = pricing_parser.extract_pricing()

# Dans save_enriched_server()
if pricing_data:
    pricing = PricingInfo(
        id=str(uuid4()),
        server_id=server.id,
        tier=pricing_data['tier'],
        currency=pricing_data.get('currency'),
        amount=pricing_data.get('amount'),
        period=pricing_data.get('period')
    )
    session.add(pricing)
```

**4. Migration database**

```bash
# Cr√©er migration
# migrations/schema/006_add_pricing_info.sql
CREATE TABLE pricing_info (
    id VARCHAR(36) PRIMARY KEY,
    server_id VARCHAR(36) UNIQUE NOT NULL,
    tier VARCHAR(20),
    currency VARCHAR(3),
    amount REAL,
    period VARCHAR(10),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (server_id) REFERENCES servers(id) ON DELETE CASCADE
);
```

**5. Tester**

```python
# Test unitaire
def test_pricing_parser():
    readme = """
    ## Pricing

    This service costs $10/month for the pro plan.
    Free tier available with limited features.
    """
    parser = PricingParser(readme)
    pricing = parser.extract_pricing()

    assert pricing['currency'] == '$'
    assert pricing['amount'] == 10.0
    assert pricing['period'] == 'month'
```

### 10.4 Debugging

**Logging d√©taill√©** :
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Inspecting database** :
```bash
sqlite3 data/mcp_servers.db
.tables
.schema servers
SELECT * FROM servers LIMIT 5;
```

**Playwright debugging (UI visible)** :
```python
scraper = BaseScraper(headless=False)  # Voir le navigateur
await scraper.navigate(url)
await asyncio.sleep(10)  # Observer manuellement
```

**Profiling performance** :
```python
import time

start = time.time()
await github_enricher.fetch_repository_info(owner, repo)
elapsed = time.time() - start
print(f"GitHub enrichment: {elapsed:.2f}s")
```

### 10.5 Tests (√Ä Impl√©menter)

**Structure sugg√©r√©e** :
```
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_readme_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tools_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ test_parameters_parser.py
‚îÇ   ‚îî‚îÄ‚îÄ test_github_enricher.py
‚îî‚îÄ‚îÄ integration/
    ‚îú‚îÄ‚îÄ test_full_pipeline.py
    ‚îî‚îÄ‚îÄ test_database_persistence.py
```

**Exemple test** :
```python
# tests/unit/test_readme_parser.py
import pytest
from src.parsers.readme_parser import ReadmeParser

def test_extract_npm_config():
    readme = """
    ## Installation

    ```bash
    npx -y @modelcontextprotocol/server-gitlab
    ```
    """
    parser = ReadmeParser(readme)
    config = parser.extract_installation_config()

    assert config is not None
    assert config['type'] == 'npm'
    assert config['command'] == 'npx'
    assert '-y' in config['args']
    assert '@modelcontextprotocol/server-gitlab' in config['args']

def test_extract_env_vars():
    readme = """
    ## Configuration

    - GITLAB_TOKEN: Your GitLab personal access token
    - GITLAB_URL: GitLab instance URL (optional)
    """
    parser = ReadmeParser(readme)
    result = parser.extract_environment_variables()

    assert 'GITLAB_TOKEN' in result['env_required']
    assert 'GITLAB_URL' in result['env_required']
    assert 'GitLab personal access token' in result['env_descriptions']['GITLAB_TOKEN']
```

---

## Annexes

### A. Commandes Utiles

```bash
# Compter lignes de code
find src/ scripts/ -name "*.py" | xargs wc -l

# Lister tous les imports
grep -r "^import\|^from" src/ scripts/ | sort | uniq

# Analyser taille base de donn√©es
du -h data/mcp_servers.db

# Compter serveurs en base
sqlite3 data/mcp_servers.db "SELECT COUNT(*) FROM servers;"

# Serveurs avec le plus de tools
sqlite3 data/mcp_servers.db "SELECT name, tools_count FROM servers ORDER BY tools_count DESC LIMIT 10;"

# Requ√™te GitHub health scores
sqlite3 data/mcp_servers.db "SELECT s.name, g.github_stars, g.github_health_score FROM servers s JOIN github_info g ON s.id = g.server_id ORDER BY g.github_health_score DESC LIMIT 10;"
```

### B. Ressources Externes

**APIs** :
- GitHub REST API: https://docs.github.com/en/rest
- npm Registry API: https://github.com/npm/registry/blob/master/docs/REGISTRY-API.md
- Playwright Python: https://playwright.dev/python/docs/intro

**Documentation** :
- SQLAlchemy: https://docs.sqlalchemy.org/
- aiohttp: https://docs.aiohttp.org/
- MCP Specification: https://modelcontextprotocol.io/

### C. Glossaire

| Terme | D√©finition |
|-------|------------|
| **MCP** | Model Context Protocol - Protocol pour connecter LLMs aux outils externes |
| **Server** | Serveur MCP fournissant des tools/resources |
| **Tool** | Fonction expos√©e par un serveur MCP (ex: get_repository) |
| **Parameter** | Argument d'un tool (ex: project_id) |
| **Enrichment** | Ajout de m√©tadonn√©es depuis sources externes (GitHub, npm) |
| **Scraping** | Extraction automatique de donn√©es depuis pages web |
| **ORM** | Object-Relational Mapping (SQLAlchemy) |
| **Async** | Programmation asynchrone (asyncio, aiohttp) |
| **Health Score** | Score 0-100 √©valuant la qualit√© d'un repository GitHub |

---

## Conclusion

Cette documentation couvre l'architecture compl√®te du MCP Hub Crawler. Pour aller plus loin :

1. **Lire le code** : Les fichiers sont bien comment√©s
2. **Suivre les imports** : Comprendre les d√©pendances
3. **Tester localement** : Ex√©cuter le pipeline avec 10 serveurs
4. **Consulter les logs** : Observer le flow en temps r√©el
5. **Examiner la base** : Requ√™tes SQL pour comprendre les donn√©es

**Questions ?** Consultez :
- `docs/PROJECT_STRUCTURE.md` - Structure du projet
- `scripts/README.md` - Guide des scripts
- `DATABASE.md` - Sch√©ma de base de donn√©es
- `CLAUDE.md` - Workflow de d√©veloppement

---

**Derni√®re mise √† jour** : 21 Novembre 2025
**Auteur** : Documentation g√©n√©r√©e avec Claude Code
